{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdRKsdPgT401",
        "outputId": "5ad0768c-00aa-4789-84ca-d1e89d4b8e6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no_penalty_simultaneous\n",
            "  f_star: 0.6500\n",
            "  a_star: 0.2197\n",
            "  b_star: 0.1517\n",
            "  rho: 0.0689\n",
            "  payoff_rider: 0.0121\n",
            "  payoff_company: 0.0198\n",
            "\n",
            "no_penalty_prior_commitment\n",
            "  f_star: 0.6500\n",
            "  a_star: 0.5096\n",
            "  b_star: 0.7040\n",
            "  rho: 0.3710\n",
            "  payoff_rider: 0.0649\n",
            "  payoff_company: 0.0357\n",
            "\n",
            "no_penalty_prior_commitment_sharing_P=0.25\n",
            "  f_star: 0.5992\n",
            "  a_star: 0.7581\n",
            "  b_star: 1.0000\n",
            "  rho: 0.7202\n",
            "  payoff_rider: 0.1078\n",
            "  payoff_company: 0.0627\n",
            "\n",
            "with_penalty_prior_commitment\n",
            "  f_star: 0.6500\n",
            "  t_star: 0.0000\n",
            "  a_star: 0.5108\n",
            "  b_star: 0.7063\n",
            "  rho: 0.3727\n",
            "  payoff_rider: 0.0652\n",
            "  payoff_company: 0.0357\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "\n",
        "@dataclass\n",
        "class Params:\n",
        "    s: float = 1.0      # company benefit per successful delivery\n",
        "    e: float = 0.30     # rider self-disbursement cost\n",
        "    d: float = 0.25     # rider effort-cost curvature\n",
        "    q: float = 0.19     # company effort-cost curvature\n",
        "    delta: float = 0.20 # effectiveness of rider's own effort (δ)\n",
        "    eta: float = 0.75   # synergy coefficient (η), with (δ+η) <= 1\n",
        "    V: float = 1.0      # max perceived loss (penalty channel)\n",
        "\n",
        "# --------- Core helpers (payoffs & responses) ---------\n",
        "\n",
        "def rho(a, b, p: Params):\n",
        "    return p.delta * a + p.eta * a * b\n",
        "\n",
        "def rider_payoff(a, b, f, p: Params):\n",
        "    # Δ_DR = (f - e) * ρ(a,b) - d * a^2\n",
        "    return (f - p.e) * rho(a, b, p) - p.d * a**2\n",
        "\n",
        "def company_payoff(a, b, f, p: Params):\n",
        "    # Δ_Q = (s - f) * ρ(a,b) - q * b^2\n",
        "    return (p.s - f) * rho(a, b, p) - p.q * b**2\n",
        "\n",
        "def rider_best_response_no_penalty(f, b, p: Params):\n",
        "    # a* = (f - e) * (δ + η b) / (2d)\n",
        "    return max(0.0, min(1.0, (f - p.e) * (p.delta + p.eta * b) / (2.0 * p.d)))\n",
        "\n",
        "def company_best_response_no_penalty(f, a, p: Params):\n",
        "    # b* = (s - f) * η a / (2q)\n",
        "    return max(0.0, min(1.0, (p.s - f) * p.eta * a / (2.0 * p.q)))\n",
        "\n",
        "# Penalty-channel helpers (multiplicative factor (1 - t/V))\n",
        "def rider_best_response_with_penalty(f, b, t, p: Params):\n",
        "    coef = (f - p.e - t) * (1.0 - t / p.V)\n",
        "    return max(0.0, min(1.0, coef * (p.delta + p.eta * b) / (2.0 * p.d)))\n",
        "\n",
        "def company_best_response_with_penalty(f, a, t, p: Params):\n",
        "    coef = (p.s - f) * (1.0 - t / p.V)\n",
        "    return max(0.0, min(1.0, coef * p.eta * a / (2.0 * p.q)))\n",
        "\n",
        "# --------- Non-penalty equilibria ---------\n",
        "\n",
        "def equilibrium_simultaneous_no_penalty(p: Params):\n",
        "    # Use fixed-point best-response iteration across f grid and pick best f for company\n",
        "    # Theory: f* = (s+e)/2 is optimal (concave Δ_Q in f) in non-penalty. :contentReference[oaicite:4]{index=4}\n",
        "    f_star = 0.5 * (p.s + p.e)\n",
        "    # Given f*, find (a,b) Nash in efforts via best responses\n",
        "    a, b = 0.2, 0.2\n",
        "    for _ in range(200):\n",
        "        a = rider_best_response_no_penalty(f_star, b, p)\n",
        "        b = company_best_response_no_penalty(f_star, a, p)\n",
        "    return {\n",
        "        \"scenario\": \"no_penalty_simultaneous\",\n",
        "        \"f_star\": f_star,\n",
        "        \"a_star\": a,\n",
        "        \"b_star\": b,\n",
        "        \"rho\": rho(a, b, p),\n",
        "        \"payoff_rider\": rider_payoff(a, b, f_star, p),\n",
        "        \"payoff_company\": company_payoff(a, b, f_star, p),\n",
        "    }\n",
        "\n",
        "def equilibrium_prior_commitment_no_penalty(p: Params):\n",
        "    # Company chooses b and f (leader); rider best-responds a.\n",
        "    # Paper indicates same f* = (s+e)/2 under prior commitment; company sets higher b than simultaneous. :contentReference[oaicite:5]{index=5}\n",
        "    f_star = 0.5 * (p.s + p.e)\n",
        "    # Choose b to maximize company payoff anticipating a(a|b,f*)\n",
        "    b_grid = np.linspace(0, 1, 1001)\n",
        "    best = None\n",
        "    for b in b_grid:\n",
        "        a = rider_best_response_no_penalty(f_star, b, p)\n",
        "        val = company_payoff(a, b, f_star, p)\n",
        "        if (best is None) or (val > best[0]):\n",
        "            best = (val, a, b)\n",
        "    val, a_star, b_star = best\n",
        "    return {\n",
        "        \"scenario\": \"no_penalty_prior_commitment\",\n",
        "        \"f_star\": f_star,\n",
        "        \"a_star\": a_star,\n",
        "        \"b_star\": b_star,\n",
        "        \"rho\": rho(a_star, b_star, p),\n",
        "        \"payoff_rider\": rider_payoff(a_star, b_star, f_star, p),\n",
        "        \"payoff_company\": val,\n",
        "    }\n",
        "\n",
        "def equilibrium_prior_commitment_with_sharing(p: Params, P: float):\n",
        "    # Sharing rider effort cost: rider's effective cost curvature becomes (1-P)*d; company also pays P*d*a^2.\n",
        "    # We adapt best response & company payoff accordingly. (Matches the 'sec' setup.) :contentReference[oaicite:6]{index=6}\n",
        "    f_star = None\n",
        "    # According to the paper, fee falls with higher sharing P; we’ll optimize f numerically in [e, s].\n",
        "    f_grid = np.linspace(p.e, p.s, 401)\n",
        "    best = None\n",
        "    for f in f_grid:\n",
        "        # leader chooses b; rider best-responds with modified cost\n",
        "        b_grid = np.linspace(0, 1, 201)\n",
        "        for b in b_grid:\n",
        "            a = max(0.0, min(1.0, (f - p.e) * (p.delta + p.eta * b) / (2.0 * ((1 - P) * p.d + 1e-12))))\n",
        "            # Company payoff now includes - q b^2 - P d a^2\n",
        "            rho_ab = rho(a, b, p)\n",
        "            payoff_company = (p.s - f) * rho_ab - p.q * b**2 - P * p.d * a**2\n",
        "            if (best is None) or (payoff_company > best[0]):\n",
        "                best = (payoff_company, f, a, b)\n",
        "    payoff_Q, f_star, a_star, b_star = best\n",
        "    payoff_r = (f_star - p.e) * rho(a_star, b_star, p) - (1 - P) * p.d * a_star**2\n",
        "    return {\n",
        "        \"scenario\": f\"no_penalty_prior_commitment_sharing_P={P:.2f}\",\n",
        "        \"f_star\": f_star,\n",
        "        \"a_star\": a_star,\n",
        "        \"b_star\": b_star,\n",
        "        \"rho\": rho(a_star, b_star, p),\n",
        "        \"payoff_rider\": payoff_r,\n",
        "        \"payoff_company\": payoff_Q,\n",
        "    }\n",
        "\n",
        "# --------- With-penalty (prior commitment on b; company sets (f, t)) ---------\n",
        "\n",
        "def equilibrium_with_penalty_prior_commitment(p: Params, f_bounds=None, t_bounds=None):\n",
        "    # Paper shows: effort responds positively to f up to a point; higher t reduces efforts for fixed f.\n",
        "    # We do a small bilevel optimization: leader chooses (f,t,b), follower chooses a.\n",
        "    # We grid-search (f,t), and for each choose b to maximize company payoff given rider best-response a.\n",
        "    # See penalty rationale & probability factor (1 - t/V). :contentReference[oaicite:7]{index=7} :contentReference[oaicite:8]{index=8}\n",
        "    if f_bounds is None:\n",
        "        f_bounds = (p.e, p.s)  # reasonable range\n",
        "    if t_bounds is None:\n",
        "        t_bounds = (0.0, p.V)\n",
        "    f_grid = np.linspace(*f_bounds, 121)\n",
        "    t_grid = np.linspace(*t_bounds, 121)\n",
        "    best = None\n",
        "    for f in f_grid:\n",
        "        for t in t_grid:\n",
        "            # company picks b (prior commit) to maximize its payoff anticipating a\n",
        "            b_grid = np.linspace(0, 1, 161)\n",
        "            for b in b_grid:\n",
        "                a = rider_best_response_with_penalty(f, b, t, p)\n",
        "                # Δ_Q^t = (s - f) * ρ(a,b) * (1 - t/V) - q b^2\n",
        "                factor = (1.0 - t / p.V)\n",
        "                val_Q = (p.s - f) * rho(a, b, p) * factor - p.q * b**2\n",
        "                if (best is None) or (val_Q > best[0]):\n",
        "                    # Δ_DR^t = (f - e - t) * ρ(a,b) * (1 - t/V) - d a^2\n",
        "                    val_R = (f - p.e - t) * rho(a, b, p) * factor - p.d * a**2\n",
        "                    best = (val_Q, val_R, f, t, a, b)\n",
        "    val_Q, val_R, f_star, t_star, a_star, b_star = best\n",
        "    return {\n",
        "        \"scenario\": \"with_penalty_prior_commitment\",\n",
        "        \"f_star\": f_star,\n",
        "        \"t_star\": t_star,\n",
        "        \"a_star\": a_star,\n",
        "        \"b_star\": b_star,\n",
        "        \"rho\": rho(a_star, b_star, p),\n",
        "        \"payoff_rider\": val_R,\n",
        "        \"payoff_company\": val_Q,\n",
        "    }\n",
        "\n",
        "# --------- Example run ---------\n",
        "if __name__ == \"__main__\":\n",
        "    p = Params()  # defaults from paper’s illustrative calibration\n",
        "    res_sim = equilibrium_simultaneous_no_penalty(p)\n",
        "    res_prior = equilibrium_prior_commitment_no_penalty(p)\n",
        "    res_share = equilibrium_prior_commitment_with_sharing(p, P=0.25)  # try 25% cost sharing\n",
        "    res_pen = equilibrium_with_penalty_prior_commitment(p)\n",
        "\n",
        "    for r in (res_sim, res_prior, res_share, res_pen):\n",
        "        print(r[\"scenario\"])\n",
        "        for k, v in r.items():\n",
        "            if k != \"scenario\":\n",
        "                print(f\"  {k}: {v:.4f}\" if isinstance(v, float) else f\"  {k}: {v}\")\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "garUZFl4T6xy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}